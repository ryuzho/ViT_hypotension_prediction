{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df291833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N of total cases: 3228\n",
      "- N of train cases: 1936\n",
      "- N of valid cases: 323\n",
      "- N of test cases: 969\n",
      "\n",
      "===== Task: classification, Seed: 93236 =====\n",
      "\n",
      "Invasive: False\n",
      "Multi: True\n",
      "Pred lag: 300\n",
      "\n",
      "N of total records: 25706\n",
      "- N of train records: 15338\n",
      "- N of valid records: 2558\n",
      "- N of test records: 7810\n",
      "\n",
      "Epoch [  1] Train loss: 0.6737 / Valid loss: 0.6365 (AUC: 0.6285) / Test loss: 0.6606 (AUC: 0.6164) < ! >\n",
      "Time: 14.1301sec\n",
      "Epoch [  2] Train loss: 0.6468 / Valid loss: 0.6373 (AUC: 0.6429) / Test loss: 0.6503 (AUC: 0.6353) < ! >\n",
      "Time: 13.9466sec\n",
      "Epoch [  3] Train loss: 0.6280 / Valid loss: 0.6191 (AUC: 0.6763) / Test loss: 0.6358 (AUC: 0.6737) < ! >\n",
      "Time: 13.9762sec\n",
      "Epoch [  4] Train loss: 0.6096 / Valid loss: 0.6004 (AUC: 0.7182) / Test loss: 0.6326 (AUC: 0.7180) < ! >\n",
      "Time: 13.9909sec\n",
      "Epoch [  5] Train loss: 0.5834 / Valid loss: 0.5763 (AUC: 0.7366) / Test loss: 0.5850 (AUC: 0.7496) < ! >\n",
      "Time: 13.9866sec\n",
      "Epoch [  6] Train loss: 0.5794 / Valid loss: 0.5710 (AUC: 0.7477) / Test loss: 0.5842 (AUC: 0.7543) < ! >\n",
      "Time: 14.2176sec\n",
      "Epoch [  7] Train loss: 0.5612 / Valid loss: 0.5763 (AUC: 0.7482) / Test loss: 0.5830 (AUC: 0.7624) < ! >\n",
      "Time: 13.9986sec\n",
      "Epoch [  8] Train loss: 0.5528 / Valid loss: 0.6376 (AUC: 0.7542) / Test loss: 0.6440 (AUC: 0.7552) < ! >\n",
      "Time: 13.9759sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.collections as collections\n",
    "import pickle, os, warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from random import randint\n",
    "\n",
    "import cv2\n",
    "from torch.autograd import Function\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from linformer import Linformer\n",
    "from vit_pytorch.efficient import ViT\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Prespecifications\n",
    "\n",
    "task = 'classification' # either 'classification' or 'regression'\n",
    "invasive = False # either True or False\n",
    "multi = True # either True or False\n",
    "pred_lag = 300 # 300 for 5-min, 600 for 10-min, 900 for 15-min prediction or others\n",
    "\n",
    "mtx_width = 300 # 50*60\n",
    "mtx_height = 3000//mtx_width\n",
    "patch_size = 10\n",
    "transformer_last_dim = 32\n",
    "\n",
    "lin_depth = 24#24 \n",
    "lin_heads = 16#16\n",
    "lin_k = 64#32\n",
    "\n",
    "if multi == True:\n",
    "    channels = 4 if invasive == True else 3\n",
    "else:\n",
    "    channels = 1\n",
    "\n",
    "cuda_number = 0 # -1 for multi GPU support\n",
    "num_workers = 0\n",
    "batch_size = 128\n",
    "max_epoch = 200\n",
    "\n",
    "train_ratio = 0.6 # Size for training dataset\n",
    "valid_ratio = 0.1 # Size for validation dataset\n",
    "test_ratio = 0.3 # Size for test dataset\n",
    "\n",
    "random_key = randint(0, 100000) # Prespecify seed number if needed\n",
    "#random_key = 35593\n",
    "\n",
    "dr_classification = 0.3 # Drop out ratio for classification model\n",
    "dr_regression = 0.0 # Drop out ratio for regression model\n",
    "\n",
    "csv_dir = './model/'+str(random_key)+'/csv/'\n",
    "pt_dir = './model/'+str(random_key)+'/pt/'\n",
    "\n",
    "if not ( os.path.isdir( csv_dir ) ):\n",
    "    os.makedirs ( os.path.join ( csv_dir ) )\n",
    "    \n",
    "if not ( os.path.isdir( pt_dir ) ):\n",
    "    os.makedirs ( os.path.join ( pt_dir ) )\n",
    "\n",
    "\n",
    "# Establish dataset\n",
    "\n",
    "class dnn_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, abp, ecg, ple, co2, target, invasive, multi):\n",
    "        self.invasive, self.multi = invasive, multi\n",
    "        self.abp, self.ecg, self.ple, self.co2 = abp, ecg, ple, co2\n",
    "        self.target = target\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.invasive == True:\n",
    "            if self.multi == True: # Invasive multi-channel model\n",
    "                return np.float32( np.vstack (( np.array ( self.abp[index] ),\n",
    "                                                np.array ( self.ecg[index] ),\n",
    "                                                np.array ( self.ple[index] ),\n",
    "                                                np.array ( self.co2[index] ) ) ) ), np.float32(self.target[index])\n",
    "            else: # Invasive mono-channel model (arterial pressure-only model)\n",
    "                return np.float32( np.array ( self.abp[index] ) ), np.float32(self.target[index])       \n",
    "        else:\n",
    "            if self.multi == True: # Non-invasive multi-channel model\n",
    "                return np.float32( np.vstack (( np.array ( self.ecg[index] ),\n",
    "                                                np.array ( self.ple[index] ),\n",
    "                                                np.array ( self.co2[index] ) ) ) ), np.float32(self.target[index])\n",
    "            else: # Non-invasive mono-channel model (photoplethysmography-only model)\n",
    "                return np.float32( np.array ( self.ple[index] ) ), np.float32(self.target[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "\n",
    "#################### Vision Transformer ##################\n",
    "\n",
    "\n",
    "    \n",
    "efficient_transformer = Linformer(\n",
    "    dim=transformer_last_dim,\n",
    "    seq_len = (mtx_width//patch_size)*(mtx_height//patch_size) + 1,  \n",
    "    depth=lin_depth, #12\n",
    "    heads=lin_heads, #8\n",
    "    k=lin_k #64\n",
    ")\n",
    "\n",
    "\n",
    "model = ViT(\n",
    "    dim=transformer_last_dim,    #128\n",
    "    image_size= max(mtx_width,mtx_height),\n",
    "    patch_size= patch_size, \n",
    "    num_classes=2,\n",
    "    transformer=efficient_transformer,\n",
    "    channels=channels, \n",
    ")\n",
    "\n",
    "###########################################################\n",
    "\n",
    "\n",
    "\n",
    "# Read dataset\n",
    "\n",
    "processed_dir = './processed_new/'\n",
    "\n",
    "file_list = np.char.split ( np.array ( os.listdir(processed_dir) ), '.' )\n",
    "case_list = []\n",
    "for caseid in file_list:\n",
    "    case_list.append ( int ( caseid[0] ) )\n",
    "print ( 'N of total cases: {}'.format ( len ( case_list ) ) )\n",
    "\n",
    "cases = {}\n",
    "cases['train'], cases['valid+test'] = train_test_split ( case_list,\n",
    "                                                        test_size=(valid_ratio+test_ratio),\n",
    "                                                        random_state=random_key )\n",
    "cases['valid'], cases['test'] = train_test_split ( cases['valid+test'],\n",
    "                                                  test_size=(test_ratio/(valid_ratio+test_ratio)),\n",
    "                                                  random_state=random_key )\n",
    "\n",
    "for phase in [ 'train', 'valid', 'test' ]:\n",
    "    print ( \"- N of {} cases: {}\".format(phase, len(cases[phase])) )\n",
    "\n",
    "for idx, caseid in enumerate(case_list):\n",
    "    filename = processed_dir + str ( caseid ) + '.pkl'\n",
    "    with open(filename, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "        data['caseid'] = [ caseid ] * len ( data['abp'] )\n",
    "        \n",
    "        raw_records = raw_records.append ( pd.DataFrame ( data ) ) if idx > 0 else pd.DataFrame ( data )\n",
    "#########################################\n",
    "############# nan 값 제거 ##############\n",
    "\n",
    "nan_list = set()\n",
    "for x in ['abp','ecg','ple','co2']:\n",
    "    j = 0\n",
    "    for i in raw_records[x]:\n",
    "        if np.isnan(i).any() == True:\n",
    "            nan_list.add(j)\n",
    "        j += 1\n",
    "    \n",
    "nan_list = list(nan_list)\n",
    "indexes_to_keep = set(range(raw_records.shape[0])) - set(nan_list)\n",
    "raw_records = raw_records.take(list(indexes_to_keep))\n",
    "\n",
    "#########################################\n",
    "raw_records = raw_records[(raw_records['map']>=20)&(raw_records['map']<=160)].reset_index(drop=True) # Exclude abnormal range\n",
    "\n",
    "\n",
    "# Define loader and model\n",
    "\n",
    "if task == 'classification':\n",
    "    task_target = 'hypo'\n",
    "    #criterion = nn.BCELoss()\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    task_target = 'map'\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "print ( '\\n===== Task: {}, Seed: {} =====\\n'.format ( task, random_key ) )\n",
    "print ( 'Invasive: {}\\nMulti: {}\\nPred lag: {}\\n'.format ( invasive, multi, pred_lag ))\n",
    "\n",
    "records = raw_records.loc[ ( raw_records['input_length']==30 ) &\n",
    "                            ( raw_records['pred_lag']==pred_lag ) ]\n",
    "\n",
    "records = records [ records.columns.tolist()[-1:] + records.columns.tolist()[:-1] ]\n",
    "print ( 'N of total records: {}'.format ( len ( records ) ))\n",
    "\n",
    "split_records = {}\n",
    "for phase in ['train', 'valid', 'test']:\n",
    "    split_records[phase] = records[records['caseid'].isin(cases[phase])].reset_index(drop=True)\n",
    "    print ('- N of {} records: {}'.format ( phase, len ( split_records[phase] )))\n",
    "\n",
    "print ( '' )\n",
    "\n",
    "ext = {}\n",
    "for phase in [ 'train', 'valid', 'test' ]:\n",
    "    ext[phase] = {}\n",
    "    for x in [ 'abp', 'ecg', 'ple', 'co2', 'hypo', 'map' ]:\n",
    "        ext[phase][x] = split_records[phase][x]\n",
    "\n",
    "dataset, loader = {}, {}\n",
    "epoch_loss, epoch_auc = {}, {}\n",
    "\n",
    "for phase in [ 'train', 'valid', 'test' ]:\n",
    "    \n",
    "#     # reshape 3000 ---> mtx_height * mtx_weight\n",
    "    ext[phase]['abp'] = [i.reshape(mtx_height,mtx_width) for i in ext[phase]['abp']]\n",
    "    ext[phase]['ecg'] = [i.reshape(mtx_height,mtx_width) for i in ext[phase]['ecg']]\n",
    "    ext[phase]['ple'] = [i.reshape(mtx_height,mtx_width) for i in ext[phase]['ple']]\n",
    "    ext[phase]['co2'] = [i.reshape(mtx_height,mtx_width) for i in ext[phase]['co2']]\n",
    "    \n",
    "    \n",
    "    dataset[phase] = dnn_dataset ( ext[phase]['abp'],\n",
    "                                    ext[phase]['ecg'],\n",
    "                                    ext[phase]['ple'],\n",
    "                                    ext[phase]['co2'],\n",
    "                                    ext[phase][task_target],\n",
    "                                    invasive = invasive, multi = multi )\n",
    "    loader[phase] = torch.utils.data.DataLoader(dataset[phase],\n",
    "                                                batch_size=batch_size,\n",
    "                                                num_workers=num_workers,\n",
    "                                                shuffle = True if phase == 'train' else False )\n",
    "    epoch_loss[phase], epoch_auc[phase] = [], []\n",
    "\n",
    "#Model development and validation\n",
    "\n",
    "torch.cuda.set_device(cuda_number)\n",
    "#DNN = Net( task = task, invasive = invasive, multi = multi )\n",
    "DNN = model\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "DNN = DNN.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(DNN.parameters(), lr=0.0005)\n",
    "n_epochs = max_epoch\n",
    "\n",
    "best_loss, best_auc = 99999.99999, 0.0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    target_stack, output_stack = {}, {}\n",
    "    current_loss, current_auc = {}, {}\n",
    "    for phase in [ 'train', 'valid', 'test' ]:\n",
    "        target_stack[phase], output_stack[phase] =  [], []\n",
    "        current_loss[phase], current_auc[phase] = 0.0, 0.0\n",
    "\n",
    "    DNN.train()\n",
    "    \n",
    "    for dnn_inputs, dnn_target in loader['train']:\n",
    "        \n",
    "        dnn_inputs, dnn_target = dnn_inputs.to(device), dnn_target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        dnn_inputs = dnn_inputs.reshape(len(dnn_inputs),channels,mtx_height,mtx_width)\n",
    "        dnn_output = DNN( dnn_inputs )\n",
    "        #import pdb; pdb.set_trace()\n",
    "        loss = criterion(dnn_output.T[0], dnn_target)\n",
    "        current_loss['train'] += loss.item()*dnn_inputs.size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    current_loss['train'] = current_loss['train']/len(loader['train'].dataset)\n",
    "    epoch_loss['train'].append ( current_loss['train'] ) \n",
    "    \n",
    "    for phase in [ 'valid', 'test']:\n",
    "    \n",
    "        DNN.eval()\n",
    "        with torch.no_grad():\n",
    "            for dnn_inputs, dnn_target in loader[phase]:\n",
    "\n",
    "                dnn_inputs, dnn_target = dnn_inputs.to(device), dnn_target.to(device)\n",
    "                dnn_inputs = dnn_inputs.reshape(len(dnn_inputs),channels,mtx_height,mtx_width)\n",
    "                dnn_output = DNN( dnn_inputs )\n",
    "                target_stack[phase].extend ( np.array ( dnn_target.cpu() ) )\n",
    "                output_stack[phase].extend ( np.array ( dnn_output.cpu().T[0] ) )\n",
    "                loss = criterion((dnn_output.T[0]), dnn_target)\n",
    "                current_loss[phase] += loss.item()*dnn_inputs.size(0)\n",
    "\n",
    "            current_loss[phase] = current_loss[phase]/len(loader[phase].dataset)\n",
    "            epoch_loss[phase].append ( current_loss[phase] ) \n",
    "\n",
    "    if task == 'classification':\n",
    "        log_label = {}\n",
    "        for phase in ['valid', 'test']:\n",
    "            current_auc[phase] = roc_auc_score ( target_stack[phase], output_stack[phase] )\n",
    "            epoch_auc[phase].append ( current_auc[phase] )\n",
    "    else:\n",
    "        reg_output, reg_target, reg_label = {}, {}, {}\n",
    "        for phase in ['valid', 'test']:\n",
    "            reg_output[phase] = np.array(output_stack[phase]).reshape(-1,1)\n",
    "            reg_target[phase] = np.array(target_stack[phase]).reshape(-1,1)\n",
    "            reg_label[phase] = np.where(reg_target[phase]<65, 1, 0)\n",
    "            method = LogisticRegression(solver='liblinear')\n",
    "            method.fit(reg_output[phase], reg_label[phase]) # Model fitting\n",
    "            current_auc[phase] = roc_auc_score (reg_label[phase], method.predict_proba(reg_output[phase]).T[1])\n",
    "            epoch_auc[phase].append ( current_auc[phase] )\n",
    "            \n",
    "            \n",
    "    label_invasive = 'invasive' if invasive == True else 'noninvasive'\n",
    "    label_multi = 'multi' if multi == True else 'mono'\n",
    "    label_pred_lag = str ( int ( pred_lag / 60 ) ) + 'min'\n",
    "\n",
    "    filename = task+'_'+label_invasive+'_'+label_multi+'_'+label_pred_lag\n",
    "\n",
    "    pd.DataFrame ( { 'train_loss':epoch_loss['train'],\n",
    "                        'valid_loss':epoch_loss['valid'],\n",
    "                        'test_loss':epoch_loss['test'],\n",
    "                        'valid_auc':epoch_auc['valid'],\n",
    "                        'test_auc':epoch_auc['test'] } ).to_csv(csv_dir+filename+'.csv')\n",
    "\n",
    "    best = ''\n",
    "    if task == 'regression' and abs(current_loss['valid']) < abs(best_loss):\n",
    "        best = '< ! >'\n",
    "        last_saved_epoch = epoch\n",
    "        best_loss = abs(current_loss['valid'])\n",
    "        #torch.save(DNN.state_dict(), pt_dir+filename+'_epoch_best.pt' )\n",
    "    elif task == 'classification' and abs(current_auc['valid']) > abs(best_auc):\n",
    "        best = '< ! >'\n",
    "        last_saved_epoch = epoch\n",
    "        best_auc = abs(current_auc['valid'])\n",
    "        #torch.save(DNN.state_dict(), pt_dir+filename+'_epoch_best.pt' )\n",
    "\n",
    "    #torch.save(DNN.state_dict(),pt_dir+filename+'_epoch_{0:03d}.pt'.format(epoch+1) )\n",
    "    \n",
    "    print ( 'Epoch [{:3d}] Train loss: {:.4f} / Valid loss: {:.4f} (AUC: {:.4f}) / Test loss: {:.4f} (AUC: {:.4f}) {}'.format\n",
    "            ( epoch+1,\n",
    "            current_loss['train'],\n",
    "            current_loss['valid'], current_auc['valid'],\n",
    "            current_loss['test'], current_auc['test'], best ) )\n",
    "    print(\"Time: {:.4f}sec\".format((time.time() - start_time)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypo_pred",
   "language": "python",
   "name": "hypo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
